# ğŸ¶ Dog Allergy Data ETL Pipeline (Databricks + PySpark)

## ğŸ“Œ Project Overview

This repository contains an **end-to-end ETL pipeline** built using **PySpark on Databricks**. The pipeline ingests dog-related data from a **public API**, cleans and transforms the JSON response, and loads the processed data into **Delta Lake tables** following the **Medallion Architecture (Bronze, Silver, Gold)**.

The project focuses on separating **allergenic** and **non-allergenic dog breeds**, optimizing Delta Lake writes, and orchestrating the complete workflow using **Databricks Jobs**.

---

## ğŸ—ï¸ Architecture (Medallion Pattern)

```
Public Dog API
      |
      v
Bronze Layer (Raw JSON Data)
      |
      v
Silver Layer (Cleaned & Structured Data)
      |
      v
Gold Layer (Allergic vs Non-Allergic Dogs)
```

---

## âš™ï¸ Tech Stack

- **Programming Language:** Python
- **Big Data Framework:** PySpark
- **Platform:** Databricks
- **Storage Format:** Delta Lake
- **Architecture Pattern:** Medallion Architecture
- **Orchestration:** Databricks Jobs

---

## ğŸ”„ ETL Workflow

### 1ï¸âƒ£ Data Ingestion (Bronze Layer)
- Fetches dog-related data from a **public API**
- Stores raw JSON responses with minimal processing
- Preserves source data for traceability and reprocessing

---

### 2ï¸âƒ£ Data Cleaning & Structuring (Silver Layer)
- Parses and normalizes JSON responses
- Handles nulls, invalid values, and inconsistent formats
- Converts cleaned data into a structured **PySpark DataFrame**

---

### 3ï¸âƒ£ Business Transformations (Gold Layer)
- Applies transformation logic to classify dog breeds into:
  - **Allergic Dogs**
  - **Non-Allergic Dogs**
- Produces analytics-ready Delta tables

---

## ğŸš€ Delta Lake Optimizations

The following optimization techniques are implemented:

- Efficient write strategies to Delta format
- Minimized small file issues
- Post-load **OPTIMIZE** operations on Delta tables
- Improved read and query performance

---

## ğŸ—„ï¸ Archive Utility

- Custom utility developed to archive temporary and intermediate files
- Helps maintain clean storage paths
- Improves operational efficiency and storage management

---

## â±ï¸ Orchestration

- The complete ETL pipeline is orchestrated using **Databricks Jobs**
- Jobs can be triggered manually or scheduled
- Ensures reliable and repeatable execution

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ bronze_ingestion.py
â”‚   â”œâ”€â”€ silver_transformation.py
â”‚   â”œâ”€â”€ gold_transformation.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ archive_utility.py
â”‚
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ databricks_job_config.json
â”‚
â”œâ”€â”€ README.md
```

---

## âœ… Key Features

- End-to-end ETL implementation
- Medallion architecture using Delta Lake
- PySpark-based transformations
- Delta Lake performance optimizations
- Custom archive utility
- Databricks Jobs orchestration

---

## ğŸ”® Future Enhancements

- Add data quality checks and validations
- Implement incremental data loading
- Add unit tests for transformations
- Integrate monitoring and alerting
- Schema enforcement and evolution

---

## ğŸ‘¤ Author

**Tamojit Das**  
Data Engineer | PySpark | Databricks | Delta Lake
